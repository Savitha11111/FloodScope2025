"""Preprocessing for the Sen1Floods11 dataset."""
from __future__ import annotations

import argparse
import csv
import json
import pathlib
from dataclasses import dataclass
from typing import Iterable, List

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import calculate_default_transform, reproject

from datasets.download_utils import record_license
from llm.cloud_coverage import calculate_cloud_coverage

LICENSE_TEXT = "Sen1Floods11 derived products generated by FloodScope2025."


@dataclass
class Scene:
    scene_id: str
    sentinel1_vv: pathlib.Path
    sentinel1_vh: pathlib.Path
    sentinel2: pathlib.Path
    flood_mask: pathlib.Path


def _ensure_dirs(root: pathlib.Path) -> tuple[pathlib.Path, pathlib.Path]:
    processed = root / "processed"
    metadata_dir = root / "metadata"
    processed.mkdir(parents=True, exist_ok=True)
    metadata_dir.mkdir(parents=True, exist_ok=True)
    return processed, metadata_dir


def _harmonise_raster(src_path: pathlib.Path, dst_path: pathlib.Path, dst_crs: str = "EPSG:4326") -> None:
    with rasterio.open(src_path) as src:
        transform, width, height = calculate_default_transform(src.crs, dst_crs, src.width, src.height, *src.bounds)
        kwargs = src.meta.copy()
        kwargs.update({"crs": dst_crs, "transform": transform, "width": width, "height": height})
        with rasterio.open(dst_path, "w", **kwargs) as dst:
            for band_index in range(1, src.count + 1):
                reproject(
                    source=rasterio.band(src, band_index),
                    destination=rasterio.band(dst, band_index),
                    src_transform=src.transform,
                    src_crs=src.crs,
                    dst_transform=transform,
                    dst_crs=dst_crs,
                    resampling=Resampling.bilinear if src.count > 1 else Resampling.nearest,
                )


def _copy_scene(scene: Scene, processed_root: pathlib.Path) -> dict:
    scene_dir = processed_root / scene.scene_id
    scene_dir.mkdir(parents=True, exist_ok=True)

    harmonised = {}
    harmonised["sentinel2_path"] = scene_dir / "sentinel2.tif"
    harmonised["sentinel1_vv_path"] = scene_dir / "sentinel1_vv.tif"
    harmonised["sentinel1_vh_path"] = scene_dir / "sentinel1_vh.tif"
    harmonised["flood_mask_path"] = scene_dir / "flood_mask.tif"

    _harmonise_raster(scene.sentinel2, harmonised["sentinel2_path"])
    _harmonise_raster(scene.sentinel1_vv, harmonised["sentinel1_vv_path"], dst_crs="EPSG:4326")
    _harmonise_raster(scene.sentinel1_vh, harmonised["sentinel1_vh_path"], dst_crs="EPSG:4326")
    _harmonise_raster(scene.flood_mask, harmonised["flood_mask_path"], dst_crs="EPSG:4326")

    cloud_fraction = calculate_cloud_coverage(str(harmonised["sentinel2_path"]))
    harmonised["cloud_fraction"] = cloud_fraction

    return harmonised


def _discover_scenes(interim_root: pathlib.Path) -> List[Scene]:
    sentinel2_files = sorted(interim_root.rglob("*S2*.tif"))
    if not sentinel2_files:
        raise FileNotFoundError("No Sentinel-2 GeoTIFFs found in the interim directory. Ensure the archive was extracted correctly.")

    scenes: List[Scene] = []
    for s2_path in sentinel2_files:
        stem = s2_path.stem.replace("_S2", "")
        s1_vv = s2_path.with_name(stem + "_S1_VV.tif")
        s1_vh = s2_path.with_name(stem + "_S1_VH.tif")
        mask = s2_path.with_name(stem + "_label.tif")
        if not (s1_vv.exists() and s1_vh.exists() and mask.exists()):
            # fall back to more permissive search
            s1_vv_candidates = list(interim_root.rglob(f"{stem}*VV*.tif"))
            s1_vh_candidates = list(interim_root.rglob(f"{stem}*VH*.tif"))
            mask_candidates = list(interim_root.rglob(f"{stem}*label*.tif"))
            if not (s1_vv_candidates and s1_vh_candidates and mask_candidates):
                continue
            s1_vv = s1_vv_candidates[0]
            s1_vh = s1_vh_candidates[0]
            mask = mask_candidates[0]
        scene_id = stem
        scenes.append(Scene(scene_id=scene_id, sentinel1_vv=s1_vv, sentinel1_vh=s1_vh, sentinel2=s2_path, flood_mask=mask))
    if not scenes:
        raise RuntimeError("Failed to match Sentinel-1/2 pairs and flood masks in Sen1Floods11 interim directory.")
    return scenes


def _write_metadata(metadata_dir: pathlib.Path, records: Iterable[dict]) -> None:
    metadata_path = metadata_dir / "sen1floods11_scenes.csv"
    fieldnames = [
        "scene_id",
        "sentinel2_path",
        "sentinel1_vv_path",
        "sentinel1_vh_path",
        "flood_mask_path",
        "cloud_fraction",
    ]
    with metadata_path.open("w", newline="", encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for record in records:
            writer.writerow(record)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Preprocess Sen1Floods11 into harmonised GeoTIFFs")
    parser.add_argument("--output-root", type=pathlib.Path, required=True)
    parser.add_argument("--interim-root", type=pathlib.Path, default=None, help="Override interim directory (defaults to <output>/interim)")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    interim_root = args.interim_root or args.output_root / "interim"
    processed_root, metadata_dir = _ensure_dirs(args.output_root)

    scenes = _discover_scenes(interim_root)
    metadata_records = []
    for scene in scenes:
        harmonised = _copy_scene(scene, processed_root)
        metadata_records.append({
            "scene_id": scene.scene_id,
            **{k: str(v) for k, v in harmonised.items() if k.endswith("_path")},
            "cloud_fraction": f"{harmonised['cloud_fraction']:.4f}",
        })

    _write_metadata(metadata_dir, metadata_records)

    coverage_report = {
        "num_scenes": len(metadata_records),
        "cloud_fraction_mean": float(np.mean([float(r["cloud_fraction"]) for r in metadata_records])),
        "cloud_fraction_std": float(np.std([float(r["cloud_fraction"]) for r in metadata_records])),
    }
    with (metadata_dir / "sen1floods11_coverage.json").open("w", encoding="utf-8") as fh:
        json.dump(coverage_report, fh, indent=2)

    record_license(args.output_root, LICENSE_TEXT)
    print(f"Processed {len(metadata_records)} Sen1Floods11 scenes into {processed_root}.")


if __name__ == "__main__":
    main()
